{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06496db3",
   "metadata": {},
   "source": [
    "# 1. Introduction\n",
    "### This section sets up main imports and logging for the RAG pipeline demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1499db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Rag Eu Lab Project: Part 1/5 ---\n",
    "# Imports and basic setup\n",
    "from __future__ import annotations\n",
    "\n",
    "import argparse\n",
    "import dataclasses\n",
    "import hashlib\n",
    "import json\n",
    "import logging\n",
    "import random\n",
    "import time\n",
    "from collections import defaultdict\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, Iterable, List, Optional, Sequence, Tuple\n",
    "\n",
    "# Optional heavy deps\n",
    "try:\n",
    "    import pandas as pd\n",
    "except Exception:\n",
    "    pd = None  # type: ignore\n",
    "\n",
    "try:\n",
    "    import numpy as np\n",
    "except Exception:\n",
    "    np = None  # type: ignore\n",
    "\n",
    "try:\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "except Exception:\n",
    "    SentenceTransformer = None\n",
    "\n",
    "try:\n",
    "    import faiss\n",
    "except Exception:\n",
    "    faiss = None\n",
    "\n",
    "try:\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    from sklearn.neighbors import NearestNeighbors\n",
    "except Exception:\n",
    "    TfidfVectorizer = None\n",
    "    NearestNeighbors = None\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='[%(asctime)s] %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd5b633",
   "metadata": {},
   "source": [
    "# 2. Data Classes\n",
    "### TableMeta describes metadata for each table.\n",
    "### DocChunk is a textual chunk derived from tables, ready for embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1688c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Rag Eu Lab Project: Part 2/5 ---\n",
    "# Data classes\n",
    "dataclass\n",
    "class TableMeta:\n",
    "    name: str\n",
    "    schema: Dict[str, str]\n",
    "    nrows: int\n",
    "    digest: str\n",
    "\n",
    "@dataclass\n",
    "class DocChunk:\n",
    "    id: str\n",
    "    table: str\n",
    "    chunk_text: str\n",
    "    metadata: Dict[str, str]\n",
    "\n",
    "# Utility function\n",
    "def short_hash(s: str, length: int = 10) -> str:\n",
    "    return hashlib.sha1(s.encode('utf-8')).hexdigest()[:length]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7e9f22",
   "metadata": {},
   "source": [
    "# 3. Table Simulation and Schema Normalizer\n",
    "### TableLoader simulates multiple tables; SchemaNormalizer converts schema and sample rows into text chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cef2325",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Rag Eu Lab Project: Part 3/5 ---\n",
    "# Table loader and schema normalizer\n",
    "class TableLoader:\n",
    "    def __init__(self, n_tables: int = 600, seed: int = 42):\n",
    "        self.n_tables = n_tables\n",
    "        random.seed(seed)\n",
    "\n",
    "    def simulate_tables(self) -> Iterable[Tuple[TableMeta, Optional[pd.DataFrame]]]:\n",
    "        for i in range(self.n_tables):\n",
    "            name = f\"eu_lab_table_{i:04d}\"\n",
    "            ncols = random.randint(5, 25)\n",
    "            nrows = random.randint(50, 5000)\n",
    "            schema = {f\"col_{c:02d}\": random.choice(['int', 'float', 'str', 'date', 'category', 'json']) for c in range(ncols)}\n",
    "            digest = short_hash(name + json.dumps(schema))\n",
    "            meta = TableMeta(name=name, schema=schema, nrows=nrows, digest=digest)\n",
    "            df = None\n",
    "            if pd is not None:\n",
    "                data = {col: [random.randint(0, 1000) if dtype=='int' else f'str_{random.randint(0,9999)}' for _ in range(min(200,nrows))] for col,dtype in schema.items()}\n",
    "                df = pd.DataFrame(data)\n",
    "            yield meta, df\n",
    "\n",
    "class SchemaNormalizer:\n",
    "    def canonicalize_schema(self, schema: Dict[str, str]) -> Dict[str, str]:\n",
    "        return {col.strip().lower().replace(' ', '_'): dtype if dtype in ['int','float','str','date','category','json'] else 'str' for col,dtype in sorted(schema.items())}\n",
    "\n",
    "    def tabular_to_text(self, meta: TableMeta, df: Optional[pd.DataFrame] = None, n_samples: int = 3) -> List[DocChunk]:\n",
    "        schema = self.canonicalize_schema(meta.schema)\n",
    "        schema_text = f\"Table: {meta.name}\\nRows: {meta.nrows}\\nSchema:\\n\" + '\\n'.join([f\" - {c}: {t}\" for c,t in schema.items()])\n",
    "        chunks = [DocChunk(id=f\"{meta.digest}_schema\", table=meta.name, chunk_text=schema_text, metadata={'role':'schema','nrows':str(meta.nrows)})]\n",
    "        if df is not None and len(df) > 0:\n",
    "            samples = df.sample(n=min(n_samples, len(df)), random_state=0)\n",
    "            for ridx,row in samples.iterrows():\n",
    "                row_text = f\"Row sample (idx={ridx}): \" + ', '.join([f\"{col}={repr(row[col])}\" for col in df.columns])\n",
    "                chunks.append(DocChunk(id=f\"{meta.digest}_row_{ridx}\", table=meta.name, chunk_text=row_text, metadata={'role':'sample_row'}))\n",
    "        stats_text = f\"Statistics for {meta.name}:\\n\" + '\\n'.join([f\" - {col}: type={t}\" for col,t in schema.items()])\n",
    "        chunks.append(DocChunk(id=f\"{meta.digest}_stats\", table=meta.name, chunk_text=stats_text, metadata={'role':'stats'}))\n",
    "        return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a168321",
   "metadata": {},
   "source": [
    "# 4. Embedding and Vector Store\n",
    "### EmbeddingModel abstracts embedding backend; VectorStore abstracts similarity search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77253968",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Rag Eu Lab Project: Part 4/5 ---\n",
    "# Embedding and Vector Store\n",
    "class EmbeddingModel:\n",
    "    def __init__(self, model_name: str = 'all-MiniLM-L6-v2'):\n",
    "        self.model = SentenceTransformer(model_name) if SentenceTransformer else None\n",
    "        self.tfidf = TfidfVectorizer(max_features=16384) if self.model is None else None\n",
    "\n",
    "    def fit_transform(self, docs: Sequence[str]):\n",
    "        if self.model: return self.model.encode(list(docs), show_progress_bar=False)\n",
    "        else: return self.tfidf.fit_transform(docs).toarray()\n",
    "\n",
    "    def transform(self, docs: Sequence[str]):\n",
    "        if self.model: return self.model.encode(list(docs), show_progress_bar=False)\n",
    "        else: return self.tfidf.transform(docs).toarray()\n",
    "\n",
    "class VectorStore:\n",
    "    def __init__(self, embedding_dim: Optional[int] = None):\n",
    "        self.embeddings = None\n",
    "        self.ids: List[str] = []\n",
    "        self.metadatas: List[Dict[str,str]] = []\n",
    "        self._index = None\n",
    "\n",
    "    def build(self, vectors, ids: List[str], metadatas: List[Dict[str,str]]):\n",
    "        import numpy as _np\n",
    "        self.embeddings = _np.array(vectors, dtype=_np.float32)\n",
    "        self.ids = ids\n",
    "        self.metadatas = metadatas\n",
    "        self._index = faiss.IndexFlatIP(len(vectors[0])) if faiss else NearestNeighbors(n_neighbors=10, metric='cosine')\n",
    "        if faiss: faiss.normalize_L2(self.embeddings); self._index.add(self.embeddings)\n",
    "        else: self._index.fit(self.embeddings)\n",
    "\n",
    "    def search(self, vectors, k: int = 5) -> List[List[Tuple[str, float]]]:\n",
    "        import numpy as _np\n",
    "        q = _np.array(vectors, dtype=_np.float32)\n",
    "        results = []\n",
    "        if faiss:\n",
    "            faiss.normalize_L2(q)\n",
    "            distances, indices = self._index.search(q,k)\n",
    "            for drow,irow in zip(distances,indices):\n",
    "                results.append([(self.ids[idx], float(dist)) for dist,idx in zip(drow,irow) if idx>=0])\n",
    "        else:\n",
    "            distances, indices = self._index.kneighbors(q, n_neighbors=k)\n",
    "            for drow,irow in zip(distances.tolist(), indices.tolist()):\n",
    "                results.append([(self.ids[idx], float(1.0-dist)) for dist,idx in zip(drow,irow)])\n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecda3842",
   "metadata": {},
   "source": [
    "# 5. Retriever, LLM, RAG Pipeline, Demo\n",
    "### Combines embeddings + vector store + stub LLM to answer queries with provenance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6861942",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Rag Eu Lab Project: Part 5/5 ---\n",
    "# Retriever + LLM + Pipeline\n",
    "class Retriever:\n",
    "    def __init__(self, embedder: EmbeddingModel):\n",
    "        self.embedder = embedder\n",
    "        self.vstore = VectorStore()\n",
    "        self.id_to_chunk: Dict[str, DocChunk] = {}\n",
    "\n",
    "    def index_documents(self, chunks: Sequence[DocChunk]):\n",
    "        texts = [c.chunk_text for c in chunks]\n",
    "        ids = [c.id for c in chunks]\n",
    "        metas = [c.metadata for c in chunks]\n",
    "        vectors = self.embedder.fit_transform(texts)\n",
    "        self.vstore.build(vectors, ids, metas)\n",
    "        self.id_to_chunk = {c.id:c for c in chunks}\n",
    "\n",
    "    def retrieve(self, query: str, k:int=5) -> List[Tuple[DocChunk,float]]:\n",
    "        qvec = self.embedder.transform([query])\n",
    "        hits = self.vstore.search(qvec,k=k)[0]\n",
    "        return [(self.id_to_chunk[hid], score) for hid,score in hits if hid in self.id_to_chunk]\n",
    "\n",
    "class LLMClient:\n",
    "    def __init__(self, provider='auto'):\n",
    "        self.have_openai = False\n",
    "\n",
    "    def answer(self, prompt: str, max_tokens: int = 256) -> str:\n",
    "        lines = [l.strip() for l in prompt.splitlines() if l.strip()]\n",
    "        salient = [l for l in lines if 'Table:' in l or ':' in l]\n",
    "        return f\"(stub-answer) Based on documents: {' '.join(salient[:6])[:800]}\"\n",
    "\n",
    "class RAGPipeline:\n",
    "    def __init__(self, retriever: Retriever, llm: LLMClient):\n",
    "        self.retriever = retriever\n",
    "        self.llm = llm\n",
    "\n",
    "    def _assemble_prompt(self, query: str, hits: List[Tuple[DocChunk,float]]) -> str:\n",
    "        header = [f\"Query: {query}\", \"---Context---\"]\n",
    "        ctx_lines = []\n",
    "        for i,(chunk,score) in enumerate(hits):\n",
    "            ctx_lines.append(f\"[Source {i+1}] table={chunk.table} id={chunk.id} score={score:.4f}\")\n",
    "            ctx_lines.append(chunk.chunk_text)\n",
    "            ctx_lines.append('---')\n",
    "        instruction = [\"Answer concisely using ONLY context.\"]\n",
    "        return '\\n'.join(header+ctx_lines+instruction)\n",
    "\n",
    "    def answer(self, query: str, k:int=5) -> Dict[str,object]:\n",
    "        hits = self.retriever.retrieve(query,k=k)\n",
    "        prompt = self._assemble_prompt(query,hits)\n",
    "        resp = self.llm.answer(prompt)\n",
    "        provenance = [{'id':c.id,'table':c.table,'score':s,'role':c.metadata.get('role','')} for c,s in hits]\n",
    "        return {'answer':resp,'provenance':provenance,'query':query}\n",
    "\n",
    "# --- Demo functions for notebook usage ---\n",
    "def build_demo_pipeline(n_tables:int=50) -> RAGPipeline:\n",
    "    loader = TableLoader(n_tables=n_tables)\n",
    "    normalizer = SchemaNormalizer()\n",
    "    embedder = EmbeddingModel()\n",
    "    retriever = Retriever(embedder)\n",
    "    all_chunks = []\n",
    "    for meta,df in loader.simulate_tables():\n",
    "        all_chunks.extend(normalizer.tabular_to_text(meta, df=df, n_samples=1))\n",
    "    retriever.index_documents(all_chunks)\n",
    "    llm = LLMClient()\n",
    "    return RAGPipeline(retriever,llm)\n",
    "\n",
    "def demo_run(pipeline:RAGPipeline, queries:Sequence[str]):\n",
    "    for q in queries:\n",
    "        out = pipeline.answer(q,k=5)\n",
    "        print(f\"\\nQuery: {q}\")\n",
    "        print(f\"Answer: {out['answer']}\")\n",
    "        print(\"Provenance:\")\n",
    "        for p in out['provenance']:\n",
    "            print(f\" - {p['id']} (table={p['table']}) score={p['score']:.4f} role={p['role']}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
